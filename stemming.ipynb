{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX7AEwJ8CPtc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer() #creating stemmer object from the class"
      ],
      "metadata": {
        "id": "zNRVYENVClYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "UBFTsAWcDqlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"going\", \"sleeping\", \"eat\", \"ate\", \"ability\", \"Meeting\"]\n",
        "for word in words:\n",
        "    print(word, \"|\", stemmer.stem(word)) # stemmer has no knowlege of language rather than fixed rules."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyUWugHKCrHH",
        "outputId": "23feb84d-f1df-426e-e8a5-1e4bd68d54da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going | go\n",
            "sleeping | sleep\n",
            "eat | eat\n",
            "ate | ate\n",
            "ability | abil\n",
            "Meeting | meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "PJRTOcifDthk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As stemmer has no knowlege of language rather than fixed rules. we wil use lemmatization to find bas word\n",
        "nlp = spacy.load(\"en_core_web_sm\") # load english language model in small letter\n",
        "doc =nlp(\" ability eats farming fishing going meeitng better\")\n",
        "for token in doc:\n",
        "    print(token,\"|\", token.lemma_, token.lemma) # token.lemma  returns hash number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTusqZohCtaf",
        "outputId": "3ba454f1-b7b9-46f9-938f-43b896a720f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  |   8532415787641010193\n",
            "ability | ability 11565809527369121409\n",
            "eats | eat 9837207709914848172\n",
            "farming | farming 12624199841640396543\n",
            "fishing | fishing 10959402079719336560\n",
            "going | go 8004577259940138793\n",
            "meeitng | meeitng 9930886417064324871\n",
            "better | well 4525988469032889948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" I am going to sleep at 11.00 clock but I usually don't go bed this time. I am very much takative\")\n",
        "for token in doc:\n",
        "  print(token, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvS2DvjoC0Gf",
        "outputId": "036ac2db-35cd-414f-d492-e940963db0c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  |  \n",
            "I | I\n",
            "am | be\n",
            "going | go\n",
            "to | to\n",
            "sleep | sleep\n",
            "at | at\n",
            "11.00 | 11.00\n",
            "clock | clock\n",
            "but | but\n",
            "I | I\n",
            "usually | usually\n",
            "do | do\n",
            "n't | not\n",
            "go | go\n",
            "bed | bed\n",
            "this | this\n",
            "time | time\n",
            ". | .\n",
            "I | I\n",
            "am | be\n",
            "very | very\n",
            "much | much\n",
            "takative | takative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As lemmatization don't work for some word like talkative, wanna, we have customize it"
      ],
      "metadata": {
        "id": "Z2oqL8R-DyCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGAkIvK9D7ss",
        "outputId": "918b2e9b-127b-4f2c-cb77-290f53ea23ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp.get_pipe('attribute_ruler') #Is used to set some rules for word/customization\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]], {\"LEMMA\":\"Brother\"})\n",
        "doc = nlp(\" Bro, you wanna go?Brah don't say no! I am exhausted\")\n",
        "for token in doc:\n",
        "  print(token.text, \"|\", token.lemma_, token.lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43isUIgREPog",
        "outputId": "ef7b8a42-c563-45df-e11b-ac1aabf577e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  |   8532415787641010193\n",
            "Bro | Brother 4347558510128575363\n",
            ", | , 2593208677638477497\n",
            "you | you 7624161793554793053\n",
            "wanna | wanna 13000462173222681081\n",
            "go?Brah | go?Brah 15250876391567038568\n",
            "do | do 2158845516055552166\n",
            "n't | not 447765159362469301\n",
            "say | say 8685289367999165211\n",
            "no | no 13055779130471031426\n",
            "! | ! 17494803046312582752\n",
            "I | I 4690420944186131903\n",
            "am | be 10382539506755952630\n",
            "exhausted | exhaust 5738807065439247694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m02fcFhqRV6w",
        "outputId": "d392bff2-bcbb-4f18-f146-1c6ebda171d1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s4_Nr1o4RtHJ",
        "outputId": "c1d940f3-dbcb-41b9-969d-49800bb5f7b3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Brother'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}